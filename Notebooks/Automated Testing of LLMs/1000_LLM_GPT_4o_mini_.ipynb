{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06hroofPAdvE"
      },
      "source": [
        "****\n",
        "\n",
        " > Installing Google Cloud Vision APIs & Installing Selenium Drivers and Libraries\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EMX8D5H46UkU",
        "outputId": "450e23e8-90c5-416a-c6c0-c7ef5503e821"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-uDXTii6rjV",
        "outputId": "9f337607-a575-45c5-9920-07f43142694d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-fd6ebcee0a81>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<module 'tensorflow._api.v2.version' from '/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/version/__init__.py'>\n",
            "GPU Available:  False\n"
          ]
        }
      ],
      "source": [
        "print(tf.version)\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ak08p8X7g04",
        "outputId": "1bfa3eb4-dc00-4e16-ade2-d3cfc03b4242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Using cached pip-24.2-py3-none-any.whl (1.8 MB)\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.2\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OgoKcB0nDuY5",
        "outputId": "cd179b52-9747-4a41-f2b0-186ecf7f7817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.24.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.7.4)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.8)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading selenium-4.24.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Installing collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.24.0 trio-0.26.2 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [962 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,562 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,264 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,145 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,435 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,498 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [53.3 kB]\n",
            "Fetched 17.2 MB in 8s (2,197 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 9 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 28.5 MB of archives.\n",
            "After this operation, 118 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.3 [595 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.63+22.04ubuntu0.1 [25.9 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 28.5 MB in 2s (11.9 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 123597 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.3_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.3) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 123805 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.63+22.04ubuntu0.1_amd64.deb ...\n",
            "Unpacking snapd (2.63+22.04ubuntu0.1) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.3) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.63+22.04ubuntu0.1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 124035 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pypixelmatch (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pypixelmatch\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Installing Packages\n",
        "!pip install selenium\n",
        "!pip install --upgrade pip\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install html5lib requests\n",
        "!pip install pypixelmatch Pillow numpy\n",
        "!pip install torch torchvision transformers pillow\n",
        "!pip install opencv-python pytesseract nltk\n",
        "!pip install google-cloud-vision openai requests pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "MQnunxyCux7i",
        "outputId": "3156f103-9f2f-4451-e607-bcb9df2c70e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Collecting google-cloud-vision\n",
            "  Downloading google_cloud_vision-3.7.4-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.43.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.19.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (3.20.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.64.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (4.9)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.6.0)\n",
            "Downloading google_cloud_vision-3.7.4-py2.py3-none-any.whl (467 kB)\n",
            "Downloading openai-1.43.0-py3-none-any.whl (365 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "Installing collected packages: jiter, httpcore, httpx, openai, google-cloud-vision\n",
            "Successfully installed google-cloud-vision-3.7.4 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.43.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "e0758998591642e1b56c2633a7e80623",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym8JdgtDBfw-"
      },
      "source": [
        "\n",
        "\n",
        "> Importing AI libraries and mounting Google Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0CqBubpJEUE",
        "outputId": "cc5d7171-9c73-4c14-f03e-301b635ef729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cat: /var/log/colab-jupyter.log: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cat /var/log/colab-jupyter.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAMvoEwhuyzt"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "from google.cloud import vision\n",
        "import openai\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSzlG7qsdfet",
        "outputId": "d3b4ae7f-6cc4-4264-fdcd-e5dc98933613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting google Drive for access to input and ouptut folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYwZQROWBn1L"
      },
      "source": [
        "\n",
        "\n",
        "> Copying JSON security file from Drive to here for using in this Notebook\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ejg6w7zbeXwy",
        "outputId": "42e4374e-2014-46f9-8558-93234708965c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/dissertation-19276544-536138fbae5d.json'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import shutil\n",
        "# JSON file management\n",
        "source_path = '/content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/dissertation-19276544-536138fbae5d.json'\n",
        "destination_path = '/content/dissertation-19276544-536138fbae5d.json'\n",
        "\n",
        "shutil.copy(source_path, destination_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skfxCtSRxxs9"
      },
      "outputs": [],
      "source": [
        "# [ℹ] Setting Google crednetials up for Using Vision API.\n",
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/dissertation-19276544-536138fbae5d.json\"\n",
        "# [ℹ] This line sets an environment variable named GOOGLE_APPLICATION_CREDENTIALS to the path of a JSON file containing your Google Cloud project's credentials.\n",
        "# [ℹ] This file typically contains information about your service account, including its private key.\n",
        "\n",
        "!export GOOGLE_APPLICATION_CREDENTIALS=\"//content/dissertation-19276544-536138fbae5d.json\"\n",
        "\n",
        "from google.cloud import vision\n",
        "\n",
        "# Setting the environment variable\n",
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/dissertation-19276544-536138fbae5d.json\"\n",
        "\n",
        "# Initialize the client\n",
        "vision_client = vision.ImageAnnotatorClient()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf8fXrXNzGjp"
      },
      "source": [
        "**The `GOOGLE_APPLICATION_CREDENTIALS` environment variable is used by Google Cloud libraries to authenticate your application to the Google Cloud platform. By setting this variable to the path of your credentials file, you're telling the Vision API to use those credentials when making requests.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q8yrLo2u6oQ"
      },
      "outputs": [],
      "source": [
        "# Setting up Google Cloud Vision API\n",
        "vision_client = vision.ImageAnnotatorClient()\n",
        "\n",
        "# Setting up OpenAI API (as a stand-in for PaLM 2, which doesn't have a public API yet)\n",
        "openai.api_key = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qVAhoPW1GYt"
      },
      "source": [
        "The ImageAnnotatorClient is the main entry point for using the Vision API.\n",
        "It provides methods for various image analysis tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgbDOGeET0Wg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T64a21ws25gs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24rW8_FSZ-_J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHlHkdqQvHqj"
      },
      "outputs": [],
      "source": [
        "# this step detects the text in Images\n",
        "def analyze_image(image_path):\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        content = image_file.read()f\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "    # [ℹ] This creates an Image object using the Google Cloud Vision API's vision.Image class. The content parameter is passed the binary data read from the image file.\n",
        "    response = vision_client.document_text_detection(image=image)\n",
        "    # [ℹ]  This line calls the document_text_detection method of the vision_client object.\n",
        "    # [ℹ]  This method sends the image object to the Google Cloud Vision API to perform OCR (Optical Character Recognition) and detect text within the image.\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83B8iqOTCPv2"
      },
      "source": [
        "\n",
        "\n",
        "> Importing Beauitful Soup Python Libaray for parsing <HTML> code generated\n",
        "\n",
        "\n",
        "\n",
        "> Importing Other Libararies for Image screenshot functionality and Images comparison and scoring\n",
        "(👇)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeirwXdIwJI2"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEckP82LD_jc",
        "outputId": "73c61c04-0732-4147-cc84-3b618fcd388d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install Pillow numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNb1xAXt6a-a"
      },
      "source": [
        "Input & Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKRb53ilLZp1"
      },
      "outputs": [],
      "source": [
        "# [ℹ️] Given Input and output folders so that I can give the input from a location into my Drive(images) and have store it my Drive(code + output images)\n",
        "input_folder = \"/content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/Input for Dataset Images/100 Images Final Folder\"\n",
        "output_folder = \"/content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/100_output\"\n",
        "# input_folder = \"/content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/Input for Dataset Images/Test input\"\n",
        "# output_folder = \"/content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/ Test results - Web gen.\"\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54CIGegHCglq"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageChops\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import base64f\n",
        "from PIL import Image\n",
        "import io\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzC1T5KxGCAs"
      },
      "outputs": [],
      "source": [
        "#  [ℹ]  Initialise the client with your API key |  a `client` object is used for interacting with OpenAI's API using the provided API key\n",
        "client = OpenAI(api_key='')  # Replace with the API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9US0X-UGF76"
      },
      "outputs": [],
      "source": [
        "# [ℹ] converting images into binary 64 code and then decoding it\n",
        "\n",
        "def image_to_base64(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        buffer = io.BytesIO()\n",
        "        img.save(buffer, format=\"PNG\")\n",
        "        return base64.b64encode(buffer.getvalue()).decode()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24bAJmxCHqtQ"
      },
      "source": [
        "\n",
        "\n",
        "> Following code takes in HTMl code, parses it using Beautiful Soup, looks for some tags returns where they are, their numbers on finding.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWfvbh9PqVEV"
      },
      "outputs": [],
      "source": [
        "# Validating HTML Code with WCAG standards\n",
        "import requests\n",
        "\n",
        "def validate_html(html_content):\n",
        "    validator_url = \"https://validator.w3.org/nu/?out=json\"\n",
        "    headers = {\n",
        "        'Content-Type': 'text/html; charset=utf-8',\n",
        "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.101 Safari/537.36'\n",
        "    }\n",
        "    response = requests.post(validator_url, headers=headers, data=html_content.encode('utf-8'))\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        errors = [msg for msg in result['messages'] if msg['type'] == 'error']\n",
        "        warnings = [msg for msg in result['messages'] if msg['type'] == 'info']\n",
        "        return len(errors) == 0, len(errors), len(warnings)\n",
        "    else:\n",
        "        return False, -1, -1  # Validation failed due to API issue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5waCfBs-N12e",
        "outputId": "40566eb8-d788-4405-8917-70299f20209f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting redis\n",
            "  Downloading redis-5.0.8-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting rq\n",
            "  Downloading rq-1.16.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis) (4.0.3)\n",
            "Requirement already satisfied: click>=5 in /usr/local/lib/python3.10/dist-packages (from rq) (8.1.7)\n",
            "Downloading redis-5.0.8-py3-none-any.whl (255 kB)\n",
            "Downloading rq-1.16.2-py3-none-any.whl (90 kB)\n",
            "Installing collected packages: redis, rq\n",
            "Successfully installed redis-5.0.8 rq-1.16.2\n"
          ]
        }
      ],
      "source": [
        "!pip install redis rq Pillow numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-ig9xTP4SkN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from PIL import Image, ImageChops\n",
        "import math\n",
        "import numpy as np\n",
        "import json\n",
        "import redis\n",
        "from rq import Queue, Worker, Connection\n",
        "import time\n",
        "\n",
        "# Redis connection\n",
        "redis_conn = redis.Redis(host='localhost', port=6379, db=0)\n",
        "queue = Queue(connection=redis_conn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjYz6ycJIcnT"
      },
      "outputs": [],
      "source": [
        "#  Parsing HTML Code using Beautiful Soup\n",
        "def analyze_html(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # 1. Check for navigation menu\n",
        "    nav_menu = soup.find('nav')\n",
        "    nav_location = \"Not found\"\n",
        "    if nav_menu:\n",
        "        nav_location = nav_menu.parent.name\n",
        "    # [ℹ️] this bit looks for <nav> tag and returns where they are and if not found then returns a text saying same.\n",
        "\n",
        "    # 2. Count sections on homepage\n",
        "    sections = soup.find_all('section')\n",
        "    section_count = len(sections)\n",
        "    # [ℹ️] this bit looks for <section> tag and returns how many they are.\n",
        "\n",
        "    # 3. Find main headline\n",
        "    main_headline = soup.find('h1')\n",
        "    headline_text = main_headline.text.strip() if main_headline else \"No main headline found\"\n",
        "    # [ℹ️] this bit looks for <h1> tag and returns where they are and if not found then returns a text saying same.\n",
        "\n",
        "    # 4. Check for footer\n",
        "    footer = soup.find('footer')\n",
        "    footer_content = footer.text.strip() if footer else \"No footer found\"\n",
        "    # [ℹ️] this bit looks for <footer> tag and returns where they are and if not found then returns a text saying same.\n",
        "\n",
        "    # 5. Count and caption images\n",
        "    images = soup.find_all('img')\n",
        "    image_count = len(images)\n",
        "    image_captions = [img.get('alt', 'No caption') for img in images]\n",
        "\n",
        "    # 6. List hyperlinks\n",
        "    links = soup.find_all('a')\n",
        "    hyperlinks = [{\"text\": link.text.strip(), \"url\": link.get('href', 'No URL')} for link in links]\n",
        "    # [ℹ️] this bit looks for <a> tag (hyperlinks) and returns where they are and if not found then returns a text saying same.\n",
        "\n",
        "    return {\n",
        "        \"nav_menu_location\": nav_location,\n",
        "        \"section_count\": section_count,\n",
        "        \"main_headline\": headline_text,\n",
        "        \"footer_content\": footer_content,\n",
        "        \"image_count\": image_count,\n",
        "        \"image_captions\": image_captions,\n",
        "        \"hyperlinks\": hyperlinks\n",
        "    }\n",
        "#   [ℹ️]  BeautifulSoup is a Python library used to parse HTML and XML documents.\n",
        "#         In the script, BeautifulSoup is used to create a soup object from the given html_content.\n",
        "#         This object allows easy navigation and searching through the HTML structure using methods like find and find_all.\n",
        "#         These methods help locate specific tags (e.g., <nav>, <section>, <h1>, <footer>, <img>, <a>)\n",
        "#         and extract their content, attributes, or parent elements. This makes it straightforward to analyze and manipulate web page data programmatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EENBYbMvZps"
      },
      "outputs": [],
      "source": [
        "# SEMATNIC Evalutation of generated HTML\n",
        "def evaluate_semantic_tags(html_content):\n",
        "    # Expected semantic tags\n",
        "    expected_tags = ['header', 'nav', 'main', 'article', 'section', 'aside', 'footer']\n",
        "\n",
        "    # Parse the HTML code\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Extract used tags\n",
        "    used_tags = [tag.name for tag in soup.find_all()]\n",
        "\n",
        "    # Calculate correctly used tags\n",
        "    correctly_used_tags = [tag for tag in used_tags if tag in expected_tags]\n",
        "\n",
        "    # Calculate precision\n",
        "    precision = len(correctly_used_tags) / len(used_tags) if used_tags else 0\n",
        "\n",
        "    # Calculate recall\n",
        "    recall = len(correctly_used_tags) / len(expected_tags)\n",
        "\n",
        "    return precision, recall, correctly_used_tags, [tag for tag in used_tags if tag not in expected_tags]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rdb00dWsJBP",
        "outputId": "de9d9992-3702-43eb-fece-e54afc862b3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Collecting pixelmatch\n",
            "  Downloading pixelmatch-0.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Downloading pixelmatch-0.3.0-py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: pixelmatch\n",
            "Successfully installed pixelmatch-0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install Pillow pixelmatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VobBaNOoyRUi"
      },
      "outputs": [],
      "source": [
        "# Using pixelmatch For Evalution\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "import pixelmatch.contrib.PIL as pixelmatch\n",
        "\n",
        "def compare_images(image1_path, image2_path):\n",
        "    # Open the images\n",
        "    image1 = Image.open(image1_path)\n",
        "    image2 = Image.open(image2_path)\n",
        "\n",
        "    # Ensure both images are the same size\n",
        "    if image1.size != image2.size:\n",
        "        # Resize the second image to match the first\n",
        "        image2 = image2.resize(image1.size)\n",
        "\n",
        "    # Create a new image for the difference\n",
        "    diff_image = Image.new(\"RGBA\", image1.size)\n",
        "\n",
        "    # Compare the images\n",
        "    mismatch = pixelmatch.pixelmatch(image1, image2, diff_image)\n",
        "\n",
        "    # Calculate the percentage of different pixels\n",
        "    total_pixels = image1.size[0] * image1.size[1]\n",
        "    diff_percentage = (mismatch / total_pixels) * 100\n",
        "\n",
        "    return mismatch, diff_percentage, diff_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D41BfI56RjPO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DFdO9oHIfi_"
      },
      "source": [
        "\n",
        "\n",
        "> This Code is a function that has a prompt message for GPT-4 LLM and it calls it using Request method and GPT-4 API and returns the HTML/CSS code for input Image(s).(👇)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIBVAsHoI3xD"
      },
      "outputs": [],
      "source": [
        "def analyze_and_generate_code(image_path):\n",
        "    base64_image = image_to_base64(image_path)\n",
        "\n",
        "    prompt = \"\"\"\n",
        "You are an expert web developer tasked with recreating a web page based on a single image. Your goal is to generate a pixel-perfect, fully functional HTML file with internal CSS that meticulously replicates the layout, styling, and functionality of the web page shown in the image. Follow these detailed instructions:\n",
        "\n",
        "1. IMAGE ANALYSIS:\n",
        "   - First, carefully analyze the provided image to determine its type:\n",
        "     a) Wireframe sketch\n",
        "     b) Long screenshot of a website (desktop view)\n",
        "     c) Proper mock-up or standard website screenshot\n",
        "   - Based on the image type, adjust your approach accordingly.\n",
        "\n",
        "2. WIREFRAME HANDLING:\n",
        "   If the image is a wireframe sketch:\n",
        "   - Recognize that a wireframe is a low-fidelity design document outlining the basic structure of the application.\n",
        "   - Identify the following common wireframe elements:\n",
        "     a) Title Elements: Rectangles with text labels (convert to appropriate <h1>, <h2>, etc. tags)\n",
        "     b) Image Elements: Squares or rectangles with an \"X\" inside (convert to <img> or <div> with background images)\n",
        "     c) Button Elements: Small rectangles with text labels (convert to <button> or <a> tags)\n",
        "     d) Input Elements: Rectangles representing input fields (convert to <input> or <textarea> tags)\n",
        "     e) Paragraph Elements: Rectangles with multiple horizontal lines (convert to <p> tags)\n",
        "   - Convert these wireframe elements into the corresponding HTML structure to build the web page's layout.\n",
        "\n",
        "3. LONG SCREENSHOT HANDLING:\n",
        "   If the image is a long screenshot of a website:\n",
        "   - Recognize that the screenshot contains multiple vertical sections representing different content and layout elements.\n",
        "   - Analyze and parse each section individually:\n",
        "     a) Header Section: Look for navigation bar, logo, and possibly a CTA button.\n",
        "     b) Hero Section: Often the first prominent section with large text, CTA, and background image/color.\n",
        "     c) Feature or Service Sections: May include text, images, and CTAs describing offerings.\n",
        "     d) Testimonials or Case Studies: Sections with quotes, client logos, or case summaries.\n",
        "     e) Footer Section: Links, contact information, and legal disclaimers.\n",
        "   - Pay attention to placeholder images:\n",
        "     - Large black plus signs (+) in white boxes indicate image placeholders.\n",
        "     - Wide plus signs represent banner images or horizontal images.\n",
        "     - Square plus signs indicate standard image sizes like logos or small visuals.\n",
        "     - Multiple plus signs together may suggest a gallery or set of icons.\n",
        "   - Generate appropriate HTML structure for each section:\n",
        "     a) Header: Use <header> with <nav>, <ul>, and other relevant tags.\n",
        "     b) Hero: Use <h1> or <h2> for large text, <a> or <button> for CTAs.\n",
        "     c) Feature Sections: Combine <section>, <div>, <h2>, <p>, and <img> tags.\n",
        "     d) Footer: Use <footer> with appropriate <ul>, <li>, and link tags.\n",
        "\n",
        "4. LAYOUT and STRUCTURE:\n",
        "   - Implement a responsive layout using appropriate CSS techniques (e.g., Flexbox, Grid) to ensure accurate positioning and spacing of all elements.\n",
        "   - Use semantic HTML5 tags (e.g., <header>, <nav>, <main>, <footer>) where applicable to enhance the structure and accessibility of the page.\n",
        "\n",
        "5. VISUAL STYLING:\n",
        "   - Extract and apply exact colors, including background colors, text colors, and any gradients or shadows.\n",
        "   - Match typography precisely, including font families, sizes, weights, and line heights. Use web-safe fonts or Google Fonts as alternatives if specific fonts can't be identified.\n",
        "   - Replicate all visual effects such as borders, rounded corners, box shadows, and hover states for interactive elements.\n",
        "\n",
        "6. CONTENT and TYPOGRAPHY:\n",
        "   - Reproduce all visible text content exactly as it appears in the image.\n",
        "   - Implement proper text alignment, spacing, and hierarchies (headings, paragraphs, lists).\n",
        "   - Use appropriate HTML elements for text content (e.g., <h1>, <p>, <ul>, <ol>, <blockquote>).\n",
        "\n",
        "7. INTERACTIVE ELEMENTS:\n",
        "   - Identify and implement all interactive components such as navigation menus, buttons, forms, and sliders.\n",
        "   - Add hover and active states for interactive elements to enhance usability.\n",
        "   - If the image suggests dropdown menus or expandable sections, implement these using CSS and minimal JavaScript if necessary.\n",
        "\n",
        "8. IMAGES and ICONS:\n",
        "   - For images in the design, use placeholder images with the same dimensions and add appropriate alt text.\n",
        "   - Implement any icons using icon fonts (e.g., Font Awesome) or SVGs to ensure scalability and performance.\n",
        "\n",
        "9. RESPONSIVE DESIGN:\n",
        "   - While the image shows a single view, implement basic responsive design principles to ensure the layout adapts to different screen sizes.\n",
        "   - Use media queries to adjust the layout for at least one mobile breakpoint.\n",
        "\n",
        "10. PERFORMANCE and ACCESSIBILITY:\n",
        "    - Optimize the CSS for performance, grouping similar styles and using shorthand properties where appropriate.\n",
        "    - Include ARIA attributes for improved accessibility where applicable.\n",
        "\n",
        "11. CODE STRUCTURE and COMMENTS:\n",
        "    - Provide a complete HTML5 structure with <!DOCTYPE html>, <html>, <head>, and <body> tags.\n",
        "    - Include all CSS within a <style> tag in the <head> section.\n",
        "    - Use clear, semantic class names that reflect the purpose or content of elements.\n",
        "    - Add detailed comments in both HTML and CSS to explain your implementation choices and any assumptions made.\n",
        "\n",
        "Output a single, complete HTML file that includes all necessary HTML, CSS, and minimal JavaScript (if required for functionality). The code should be clean, well-formatted, and adhere to modern web development best practices. Aim for a result that, when opened in a web browser, looks as close to the original image as possible.\n",
        "\n",
        "Example HTML file structure:\n",
        "\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Web Page from Image</title>\n",
        "    <style>\n",
        "        /* CSS styles here */\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <!-- HTML structure here -->\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "   \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
        "                        }\n",
        "                    }\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        max_tokens=2000\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FJFFs4zfMC6k",
        "outputId": "1a1c861a-cbbc-4499-e4d4-34f96804ded1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (3,505 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 124063 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install tesseract-ocr\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGaMmV20pZxr"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9V7EvkvpsOc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKxqYZE5MeAH"
      },
      "outputs": [],
      "source": [
        "# [ℹ️] this code formats the code so that Styles(CSS) are inside HTML Code for ease of opening and evaluting code\n",
        "# HTML template\n",
        "html_template = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>{title}</title>\n",
        "    <style>\n",
        "        {generated_code}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    {generated_body}\n",
        "</body>\n",
        "</html>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDu6a8fKMuJE"
      },
      "source": [
        "\n",
        "\n",
        "> Defining functions to capture Screenshots from generated HTML/CSS code using Selenium and comparing the original Image input using SSIM score for evalution (👇)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNs7VMfzN83-"
      },
      "outputs": [],
      "source": [
        "def capture_screenshot(html_path, output_path):\n",
        "    options = Options()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    driver.get(f'file://{html_path}')\n",
        "    driver.save_screenshot(output_path)\n",
        "    driver.quit()\n",
        "\n",
        "def calculate_ssim(image1_path, image2_path):\n",
        "    img1 = Image.open(image1_path).convert('L')\n",
        "    img2 = Image.open(image2_path).convert('L')\n",
        "    img1 = np.array(img1)\n",
        "    img2 = np.array(img2)\n",
        "\n",
        "    # Resize images if they have different dimensions\n",
        "    if img1.shape != img2.shape:\n",
        "        img2 = np.array(Image.fromarray(img2).resize(img1.shape[::-1]))\n",
        "    return ssim(img1, img2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRHE-iH5OUPc"
      },
      "source": [
        "\n",
        "\n",
        "> Setting files together and executing the above functions as needed\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7ymh3KZpTRO"
      },
      "outputs": [],
      "source": [
        "# Using CLIP Score for Evalution\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "def calculate_clip_score(image1_path, image2_path):\n",
        "    # Load the CLIP model and processor\n",
        "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "    # Open and preprocess the images\n",
        "    image1 = Image.open(image1_path).convert('RGB')\n",
        "    image2 = Image.open(image2_path).convert('RGB')\n",
        "\n",
        "    # Preprocess the images\n",
        "    inputs = processor(images=[image1, image2], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Get the image features\n",
        "    with torch.no_grad():\n",
        "        image_features = model.get_image_features(**inputs)\n",
        "\n",
        "    # Calculate the cosine similarity\n",
        "    similarity = torch.nn.functional.cosine_similarity(image_features[0], image_features[1], dim=0)\n",
        "    return similarity.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnIxIg5ixtqF"
      },
      "source": [
        "\n",
        "\n",
        "> **Process all the Files in Smaller Batch Windows**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqW_1Rvfno_G"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import json\n",
        "import gc\n",
        "import warnings\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "Image.MAX_IMAGE_PIXELS = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAELzPJzVVIt"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "INDIVIDUAL_IMAGES_FOLDER = \"/content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/Final Folder with all four datasets\"\n",
        "ZIP_FILES_FOLDER = \"/content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/1000_zip_ten\"\n",
        "OUTPUT_FOLDER = \"/content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/Output_final_1000\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTE0eYz3VW5D"
      },
      "outputs": [],
      "source": [
        "zip_folder_path = \"/content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/1000_zip_ten\"\n",
        "output_folder_path = \"/content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/Output_final_1000\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_folder_path, exist_ok=True)\n",
        "\n",
        "# List all ZIP files\n",
        "zip_files = sorted([f for f in os.listdir(zip_folder_path) if f.endswith('.zip')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6KGo0RPQ7-B"
      },
      "outputs": [],
      "source": [
        "# Memory log\n",
        "def log_memory_usage(batch_number, image_number):\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_info = process.memory_info()\n",
        "    with open(os.path.join(OUTPUT_FOLDER, 'memory_log.txt'), 'a') as f:\n",
        "        f.write(f\"Batch {batch_number}, Image {image_number}: {memory_info.rss / 1024 / 1024:.2f} MB\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6tTXVZkQ_17"
      },
      "outputs": [],
      "source": [
        "def process_single_image(image_path, output_folder):\n",
        "    image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "    print(f\"Processing {image_name}\")\n",
        "\n",
        "    # Check image dimensions\n",
        "    with Image.open(image_path) as img:\n",
        "        width, height = img.size\n",
        "        aspect_ratio = height / width\n",
        "        if width > 3000 or height > 40000 or aspect_ratio > 20:  # Added width check\n",
        "            print(f\"Skipping {image_name} due to excessive width, height, or aspect ratio\")\n",
        "            return None\n",
        "\n",
        "    # Generate code (placeholder function)\n",
        "    generated_code = analyze_and_generate_code(image_path)\n",
        "\n",
        "    # Extract CSS and body from generated code\n",
        "    css_start = generated_code.find('<style>')\n",
        "    css_end = generated_code.find('</style>')\n",
        "    body_start = generated_code.find('<body>')\n",
        "    body_end = generated_code.find('</body>')\n",
        "\n",
        "    css = generated_code[css_start+7:css_end] if css_start != -1 and css_end != -1 else ''\n",
        "    body = generated_code[body_start+6:body_end] if body_start != -1 and body_end != -1 else generated_code\n",
        "\n",
        "    # Create HTML content (placeholder variable)\n",
        "    html_content = html_template.format(\n",
        "        title=image_name,\n",
        "        generated_code=css,\n",
        "        generated_body=body\n",
        "    )\n",
        "\n",
        "    # Save individual HTML file\n",
        "    output_file_path = os.path.join(output_folder, f\"{image_name}.html\")\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    # Capture screenshot (placeholder function)\n",
        "    screenshot_path = os.path.join(output_folder, f\"{image_name}_screenshot.png\")\n",
        "    capture_screenshot(output_file_path, screenshot_path)\n",
        "\n",
        "    # Calculate metrics (placeholder functions)\n",
        "    ssim_score = calculate_ssim(image_path, screenshot_path)\n",
        "    _, diff_percentage, diff_image = compare_images(image_path, screenshot_path)\n",
        "    clip_score = calculate_clip_score(image_path, screenshot_path)\n",
        "    is_valid, _, _ = validate_html(html_content)\n",
        "    precision, recall, _, _ = evaluate_semantic_tags(html_content)\n",
        "\n",
        "    # Save difference image\n",
        "    diff_image_path = os.path.join(output_folder, f\"{image_name}_diff.png\")\n",
        "    diff_image.save(diff_image_path)\n",
        "\n",
        "    # Collect results\n",
        "    image_results = {\n",
        "        'Image': image_name,\n",
        "        'SSIM': ssim_score,\n",
        "        'PixelMatch_DiffPercentage': diff_percentage,\n",
        "        'CLIP': clip_score,\n",
        "        'HTML_Valid': is_valid,\n",
        "        'Semantic_Precision': precision,\n",
        "        'Semantic_Recall': recall,\n",
        "    }\n",
        "\n",
        "    return image_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0GWKiRnRC3-"
      },
      "outputs": [],
      "source": [
        "def process_zip(zip_path, output_folder):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_name = os.path.splitext(os.path.basename(zip_path))[0]\n",
        "        extract_folder = os.path.join(output_folder, f\"{zip_name}_extracted\")\n",
        "\n",
        "        os.makedirs(extract_folder, exist_ok=True)\n",
        "        zip_ref.extractall(extract_folder)\n",
        "\n",
        "        results = []\n",
        "        # Get all image files in the extracted folder\n",
        "        image_files = sorted([f for f in os.listdir(extract_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))])\n",
        "\n",
        "        # Process images in batches of 10\n",
        "        for i in range(0, len(image_files), 10):\n",
        "            batch = image_files[i:i+10]\n",
        "            batch_folder = os.path.join(output_folder, f\"{zip_name}_batch_{i//10+1}\")\n",
        "            os.makedirs(batch_folder, exist_ok=True)\n",
        "\n",
        "            for filename in batch:\n",
        "                input_path = os.path.join(extract_folder, filename)\n",
        "                result = process_single_image(input_path, batch_folder)\n",
        "                if result:\n",
        "                    results.append(result)\n",
        "\n",
        "        shutil.rmtree(extract_folder)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKFUzeShRKNr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def save_checkpoint(checkpoint_file, processed_items):\n",
        "    with open(checkpoint_file, 'w') as f:\n",
        "        json.dump({'processed_items': processed_items}, f)\n",
        "\n",
        "def load_checkpoint(checkpoint_file):\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        with open(checkpoint_file, 'r') as f:\n",
        "            return json.load(f)['processed_items']\n",
        "    return []\n",
        "\n",
        "def enqueue_tasks(individual_folder, zip_folder, output_folder, checkpoint_file):\n",
        "    all_images = glob.glob(os.path.join(individual_folder, \"*.png\"))\n",
        "    all_zips = glob.glob(os.path.join(zip_folder, \"*.zip\"))\n",
        "    processed_items = load_checkpoint(checkpoint_file)\n",
        "\n",
        "    for image_path in all_images:\n",
        "        if image_path not in processed_items:\n",
        "            queue.enqueue(process_single_image, image_path, output_folder)\n",
        "\n",
        "    for zip_path in all_zips:\n",
        "        if zip_path not in processed_items:\n",
        "            queue.enqueue(process_zip, zip_path, output_folder)\n",
        "\n",
        "    return len(all_images) + len(all_zips) - len(processed_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InRZrqpyRQwl"
      },
      "outputs": [],
      "source": [
        "def main_processing():\n",
        "    individual_images_folder = input(\"Enter the path to the folder with individual images: \").strip()\n",
        "    zip_files_folder = input(\"Enter the path to the folder with ZIP files: \").strip()\n",
        "    output_folder = input(\"Enter the path to the output folder: \").strip()\n",
        "\n",
        "    create_directory(output_folder)\n",
        "    checkpoint_file = os.path.join(output_folder, \"checkpoint.json\")\n",
        "\n",
        "    try:\n",
        "        processed_items = load_checkpoint(checkpoint_file)\n",
        "        all_images = glob.glob(os.path.join(individual_images_folder, \"*.png\"))\n",
        "        all_zips = glob.glob(os.path.join(zip_files_folder, \"*.zip\"))\n",
        "\n",
        "        total_items = len(all_images) + len(all_zips)\n",
        "        processed_count = len(processed_items)\n",
        "\n",
        "        print(f\"Found {len(all_images)} individual images and {len(all_zips)} ZIP files.\")\n",
        "        print(f\"Already processed: {processed_count} items.\")\n",
        "\n",
        "        for image_path in tqdm(all_images, desc=\"Processing individual images\"):\n",
        "            if image_path not in processed_items:\n",
        "                result = process_single_image(image_path, output_folder)\n",
        "                if result:\n",
        "                    with open(os.path.join(output_folder, f\"{os.path.splitext(os.path.basename(image_path))[0]}_results.json\"), 'w') as f:\n",
        "                        json.dump(result, f)\n",
        "                    processed_items.append(image_path)\n",
        "                    save_checkpoint(checkpoint_file, processed_items)\n",
        "                    processed_count += 1\n",
        "\n",
        "        for zip_path in tqdm(all_zips, desc=\"Processing ZIP files\"):\n",
        "            if zip_path not in processed_items:\n",
        "                results = process_zip(zip_path, output_folder)\n",
        "                for result in results:\n",
        "                    with open(os.path.join(output_folder, f\"{result['Image']}_results.json\"), 'w') as f:\n",
        "                        json.dump(result, f)\n",
        "                processed_items.append(zip_path)\n",
        "                save_checkpoint(checkpoint_file, processed_items)\n",
        "                processed_count += 1\n",
        "\n",
        "        print(\"All items processed successfully!\")\n",
        "\n",
        "        # Combine all results into a single JSONL file\n",
        "        final_results_file = os.path.join(output_folder, 'all_results.jsonl')\n",
        "        with open(final_results_file, 'w') as outfile:\n",
        "            for item in processed_items:\n",
        "                result_file = os.path.join(output_folder, f\"{os.path.splitext(os.path.basename(item))[0]}_results.json\")\n",
        "                if os.path.exists(result_file):\n",
        "                    with open(result_file, 'r') as infile:\n",
        "                        outfile.write(infile.read() + '\\n')\n",
        "\n",
        "        print(f\"All results combined in {final_results_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        print(\"Please check your input and try again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Js6w9FURUPh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_results(folder_name, output_folder):\n",
        "    results_zip = os.path.join(output_folder, f\"{folder_name}_results.zip\")\n",
        "    if not os.path.exists(results_zip):\n",
        "        print(f\"No results found for {folder_name}\")\n",
        "        return\n",
        "\n",
        "    with zipfile.ZipFile(results_zip, 'r') as zip_ref:\n",
        "        temp_folder = os.path.join(output_folder, \"temp_results\")\n",
        "        zip_ref.extractall(temp_folder)\n",
        "\n",
        "        results = []\n",
        "        for filename in os.listdir(temp_folder):\n",
        "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
        "                results.append({\"path\": os.path.join(temp_folder, filename)})\n",
        "\n",
        "        for i in range(0, len(results), 2):\n",
        "            if i + 1 < len(results):\n",
        "                img1 = results[i]\n",
        "                img2 = results[i + 1]\n",
        "                diff, score = compare_images(img1['path'], img2['path'])\n",
        "\n",
        "                print(f\"Image 1: {os.path.basename(img1['path'])}\")\n",
        "                print(f\"Image 2: {os.path.basename(img2['path'])}\")\n",
        "                print(f\"Difference score: {score}\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "        shutil.rmtree(temp_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "collapsed": true,
        "id": "OhE2t3mLxs8v",
        "outputId": "cec8c870-976c-426a-86f3-786ab50b112c"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-e988d0336949>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmain_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-f731fd0b8f59>\u001b[0m in \u001b[0;36mmain_processing\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mindividual_images_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the path to the folder with individual images: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mzip_files_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the path to the folder with ZIP files: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the path to the output folder: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import psutil\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageChops\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "import shutil\n",
        "import redis\n",
        "from rq import Queue, Connection, Worker\n",
        "\n",
        "def create_directory(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "        print(f\"Created directory: {directory}\")\n",
        "    else:\n",
        "        print(f\"Directory already exists: {directory}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_processing()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or3ey3sVBbJh"
      },
      "source": [
        "# **Code Generation Ended.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5dq1B-f3pIFS"
      },
      "outputs": [],
      "source": [
        "# Code for converting JSON files into one CSV file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Ur-JEdozz7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "653sn-PMMbok",
        "outputId": "16e3332e-4e1e-4c23-ae67-29cc2ac5a449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the path to the folder containing .json files: /content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/The Output Folder/Gemini_Final1000/all_results.jsonl\n",
            "Enter the path for the output CSV file or directory: /content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/The Output Folder\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files: 0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file created: /content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/The Output Folder/image_metrics_output.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "def json_to_csv(input_folder, output_path):\n",
        "    # Get all .json files in the input folder\n",
        "    json_files = glob.glob(os.path.join(input_folder, '*.json'))\n",
        "\n",
        "    # If output_path is a directory, create a file name\n",
        "    if os.path.isdir(output_path):\n",
        "        output_csv = os.path.join(output_path, 'image_metrics_output.csv')\n",
        "    else:\n",
        "        output_csv = output_path\n",
        "\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
        "\n",
        "    # Prepare CSV writer\n",
        "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['Image Name']\n",
        "        csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')\n",
        "        csv_writer.writeheader()\n",
        "\n",
        "        # Process each .json file\n",
        "        for json_file in tqdm(json_files, desc=\"Processing JSON files\"):\n",
        "            try:\n",
        "                with open(json_file, 'r', encoding='utf-8') as file:\n",
        "                    data = json.load(file)\n",
        "\n",
        "                # Update fieldnames if new fields are encountered\n",
        "                new_fields = [key for key in data.keys() if key not in fieldnames and key != 'Image']\n",
        "                if new_fields:\n",
        "                    fieldnames.extend(new_fields)\n",
        "                    csv_writer.fieldnames = fieldnames\n",
        "                    csv_writer.writeheader()\n",
        "\n",
        "                # Prepare the row data\n",
        "                row_data = {'Image Name': os.path.basename(json_file).replace('_results.json', '')}\n",
        "                row_data.update({key: value for key, value in data.items() if key != 'Image'})\n",
        "\n",
        "                # Write the row\n",
        "                csv_writer.writerow(row_data)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {json_file}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"CSV file created: {output_csv}\")\n",
        "\n",
        "# Usage\n",
        "input_folder = input(\"Enter the path to the folder containing .json files: \").strip()\n",
        "output_path = input(\"Enter the path for the output CSV file or directory: \").strip()\n",
        "\n",
        "json_to_csv(input_folder, output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5RV0owJfp-oe"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install tensorflow # Install TensorFlow if you haven't already.\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "5qkhxHAIIzdt",
        "outputId": "f6730179-9251-4a43-bfd6-646edb58f89c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All data has been written to /content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/The Output Folder/Gemini-1.5-pro_metrics.csv\n",
            "\n",
            "Initiating download of the CSV file...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_5d90ab82-1114-4bcb-99e0-7c51eb9bb046\", \"Gemini-1.5-pro_metrics.csv\", 87257)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "def read_jsonl(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return [json.loads(line) for line in file]\n",
        "\n",
        "def convert_jsonl_to_csv(jsonl_file, csv_file):\n",
        "    data = read_jsonl(jsonl_file)\n",
        "\n",
        "    if not data:\n",
        "        print(\"No data found in the JSONL file.\")\n",
        "        return\n",
        "\n",
        "    fieldnames = list(data[0].keys())\n",
        "\n",
        "    with open(csv_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for item in data:\n",
        "            writer.writerow(item)\n",
        "\n",
        "    print(f\"\\nAll data has been written to {csv_file}\")\n",
        "\n",
        "def download_file(file_path):\n",
        "    files.download(file_path)\n",
        "\n",
        "# Usage\n",
        "jsonl_file = \"/content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/The Output Folder/Gemini_Final1000/all_results.jsonl\"\n",
        "csv_file = \"/content/drive/MyDrive/Dissertation Aditya Parmar : testing of Large Lang. Model/The Output Folder/Gemini-1.5-pro_metrics.csv\"\n",
        "\n",
        "# Convert JSONL to CSV\n",
        "convert_jsonl_to_csv(jsonl_file, csv_file)\n",
        "\n",
        "# Download the CSV file\n",
        "print(\"\\nInitiating download of the CSV file...\")\n",
        "download_file(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZUCbqGGgF2o"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
